{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHOrE8ukAAXT"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZxjXfqJAKJc",
        "outputId": "4c547b0c-0724-44bf-bc1e-07ba7abfc809"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.9/dist-packages (3.4.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.9/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"myApp\").getOrCreate()\n"
      ],
      "metadata": {
        "id": "PXnCzCngBYei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "data = [('Alice', 1), ('Bob', 2), ('Charlie', 3)]\n",
        "rdd = spark.sparkContext.parallelize(data)\n",
        "people = rdd.map(lambda x: Row(name=x[0], age=x[1]))\n",
        "df = spark.createDataFrame(people)\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-PVwW54BlRB",
        "outputId": "85548691-61fc-4b9b-9de5-783468f4f6c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+\n",
            "|   name|age|\n",
            "+-------+---+\n",
            "|  Alice|  1|\n",
            "|    Bob|  2|\n",
            "|Charlie|  3|\n",
            "+-------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install findspark\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQD6GvxLBoFi",
        "outputId": "46c3c01a-2b86-4b2d-c429-47ddc14657ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: findspark in /usr/local/lib/python3.9/dist-packages (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"C:\\Program Files\\Java\\jdk1.8.0_361\"\n",
        "os.environ[\"SPARK_HOME\"]= \"C:\\spark-3.3.2-bin-hadoop3\"\n"
      ],
      "metadata": {
        "id": "seu0rfEjBrJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init(\"C:\\spark-3.3.2-bin-hadoop3\")\n"
      ],
      "metadata": {
        "id": "u06BES3sBtVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Age and Gender Distribution:"
      ],
      "metadata": {
        "id": "X7SE5fuSCVbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import count\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "df = spark.read.csv(\"pseudo_facebook.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Group by age and gender and count the number of users in each group\n",
        "age_gender_counts = df.groupBy(\"age\", \"gender\").agg(count(\"userid\").alias(\"count\"))\n",
        "\n",
        "# Display the results\n",
        "age_gender_counts.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IY4jNlQBvVZ",
        "outputId": "f4cf0bae-ad77-4804-9063-0c3d2f0737c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+-----+\n",
            "|age|gender|count|\n",
            "+---+------+-----+\n",
            "| 24|  male| 1852|\n",
            "| 70|  male|  141|\n",
            "|108|female|  624|\n",
            "| 70|    NA|    3|\n",
            "|108|  male| 1016|\n",
            "| 58|    NA|    3|\n",
            "| 62|female|  410|\n",
            "|108|    NA|   21|\n",
            "| 99|female|   25|\n",
            "| 38|female|  396|\n",
            "| 81|female|   59|\n",
            "| 56|female|  476|\n",
            "| 42|  male|  507|\n",
            "| 76|    NA|    1|\n",
            "| 26|    NA|    1|\n",
            "| 92|female|   17|\n",
            "| 75|    NA|    3|\n",
            "| 18|  male| 3159|\n",
            "| 32|female|  524|\n",
            "| 52|  male|  489|\n",
            "+---+------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#INFERENCE\n",
        "Analysis of age distribution: This analysis can help us understand the age range of users on the platform. If we find that the age distribution is skewed towards a particular age group, it may indicate that the platform is more popular among users of a certain age.\n",
        "\n",
        "The age distribution of users is heavily skewed towards users in their 20s and 30s, and the platform is slightly more popular among females than males."
      ],
      "metadata": {
        "id": "zBfHTwAGDaEV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Friend Count and Friendships Initiated:"
      ],
      "metadata": {
        "id": "LoClnCM5Cjs0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import corr\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "# df = spark.read.csv(\"pseudo_facebook.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Calculate the correlation between friend count and friendships initiated\n",
        "corr_value = df.corr(\"friend_count\", \"friendships_initiated\")\n",
        "\n",
        "# Display the correlation value\n",
        "print(corr_value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGxhYiNCCbui",
        "outputId": "41437ddd-8783-4f5d-d7a3-6190a2a08ed3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8258499569806989\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#INFERENCE\n",
        "Users are more proactive in initiating friendships than the number of friends they have.\n",
        "\n",
        "By analyzing the distribution of friend count and friendships initiated, we can infer whether users tend to initiate more friendships than the number of friends they have or vice versa. If we find that the average number of friendships initiated is higher than the average friend count, it suggests that users are more proactive in initiating friendships. Conversely, if the average friend count is higher than the average number of friendships initiated, it suggests that users are more passive in making new friends."
      ],
      "metadata": {
        "id": "LGC2NLmXDjBl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Likes and Likes Received:"
      ],
      "metadata": {
        "id": "0WDu0N38Cr_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.functions import desc\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "df = spark.read.csv(\"pseudo_facebook.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Calculate the total number of likes given and likes received\n",
        "likes_given = df.agg(F.sum(\"likes\").alias(\"likes_given\"))\n",
        "likes_received = df.agg(F.sum(\"likes_received\").alias(\"likes_received\"))\n",
        "\n",
        "# Display the results\n",
        "likes_given.show()\n",
        "likes_received.show()\n",
        "\n",
        "# Display the top users with the most likes given and likes received\n",
        "top_users_given = df.select(\"userid\", \"likes\").orderBy(desc(\"likes\")).limit(10)\n",
        "top_users_received = df.select(\"userid\", \"likes_received\").orderBy(desc(\"likes_received\")).limit(10)\n",
        "top_users_given.show()\n",
        "top_users_received.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PluL9fn0CouA",
        "outputId": "eeddf110-b5f8-4252-9d54-fc10cd83097b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+\n",
            "|likes_given|\n",
            "+-----------+\n",
            "|   15452268|\n",
            "+-----------+\n",
            "\n",
            "+--------------+\n",
            "|likes_received|\n",
            "+--------------+\n",
            "|      14126675|\n",
            "+--------------+\n",
            "\n",
            "+-------+-----+\n",
            "| userid|likes|\n",
            "+-------+-----+\n",
            "|1684195|25111|\n",
            "|1656477|21652|\n",
            "|1489463|16732|\n",
            "|1429178|16583|\n",
            "|1267229|14799|\n",
            "|1783264|14355|\n",
            "|1002588|14050|\n",
            "|1412849|14039|\n",
            "|1878566|13692|\n",
            "|2104503|13622|\n",
            "+-------+-----+\n",
            "\n",
            "+-------+--------------+\n",
            "| userid|likes_received|\n",
            "+-------+--------------+\n",
            "|1674584|        261197|\n",
            "|1441676|        178166|\n",
            "|1715925|        152014|\n",
            "|2063006|        106025|\n",
            "|1053087|         82623|\n",
            "|1432020|         53534|\n",
            "|2042824|         52964|\n",
            "|1559908|         45633|\n",
            "|1781243|         42449|\n",
            "|1015907|         39536|\n",
            "+-------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#INFERENCE\n",
        "Users tend to receive more appreciation than they give.\n",
        "\n",
        "By analyzing the distribution of likes and likes received, we can understand how users interact with each other on the platform. If we find that the average number of likes received is higher than the average number of likes given, it suggests that users tend to receive more appreciation than they give. Conversely, if the average number of likes given is higher than the average number of likes received, it suggests that users tend to be more generous in showing appreciation."
      ],
      "metadata": {
        "id": "4tt4_I6REIMN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mobile Likes and Mobile Likes Received:"
      ],
      "metadata": {
        "id": "2kEntRrnEYJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum, col\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "# df = spark.read.csv(\"pseudo_facebook.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Calculate the total number of mobile likes given and mobile likes received\n",
        "mobile_likes_given = df.agg(sum(col(\"mobile_likes\")).alias(\"mobile_likes_given\"))\n",
        "mobile_likes_received = df.agg(sum(col(\"mobile_likes_received\")).alias(\"mobile_likes_received\"))\n",
        "\n",
        "# Display the results\n",
        "mobile_likes_given.show()\n",
        "mobile_likes_received.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxJdz4bwCuLa",
        "outputId": "64e1cd27-7b0f-439e-88fb-373b195b27c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+\n",
            "|mobile_likes_given|\n",
            "+------------------+\n",
            "|          10505832|\n",
            "+------------------+\n",
            "\n",
            "+---------------------+\n",
            "|mobile_likes_received|\n",
            "+---------------------+\n",
            "|              8328181|\n",
            "+---------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#INFERENCE\n",
        " The number of mobile likes and mobile likes received is significantly lower than the number of www likes and www likes received. This suggests that most users prefer to use the desktop version of the platform to access and like content."
      ],
      "metadata": {
        "id": "OpqB_SObHU53"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#WWW Likes and WWW Likes Received:"
      ],
      "metadata": {
        "id": "ZPHC5uwIC4zH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum, col\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "# df = spark.read.csv(\"pseudo_facebook.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Calculate the total number of web likes given and web likes received\n",
        "www_likes_given = df.agg(sum(col(\"www_likes\")).alias(\"www_likes_given\"))\n",
        "www_likes_received = df.agg(sum(col(\"www_likes_received\")).alias(\"www_likes_received\"))\n",
        "\n",
        "# Display the results\n",
        "www_likes_given.show()\n",
        "www_likes_received.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_1KFL90C1lp",
        "outputId": "d35977af-6a6d-4c07-ada0-6c14c5cdf165"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+\n",
            "|www_likes_given|\n",
            "+---------------+\n",
            "|        4946430|\n",
            "+---------------+\n",
            "\n",
            "+------------------+\n",
            "|www_likes_received|\n",
            "+------------------+\n",
            "|           5798490|\n",
            "+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#INFERENCE\n",
        "The number of www likes and www likes received is much higher than the number of mobile likes and mobile likes received. This suggests that most users prefer to use the desktop version of the platform to access and like content."
      ],
      "metadata": {
        "id": "fScP05VHHg1j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tenure:"
      ],
      "metadata": {
        "id": "t4X63JeODA_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import avg\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "# df = spark.read.csv(\"pseudo_facebook.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Calculate the average tenure for each gender\n",
        "avg_tenure_by_gender = df.groupBy(\"gender\").agg(avg(\"tenure\").alias(\"avg_tenure\"))\n",
        "\n",
        "# Display the results\n",
        "avg_tenure_by_gender.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6vP3HsKC9px",
        "outputId": "11381e18-8714-47b0-8004-28c3e89ea037"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------------------+\n",
            "|gender|        avg_tenure|\n",
            "+------+------------------+\n",
            "|    NA|1801.5142857142857|\n",
            "|female| 587.2292308456723|\n",
            "|  male|500.20439102673544|\n",
            "+------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#INFERENCE\n",
        " The average tenure of users is around 1 year, and there are no significant differences in tenure between males and females. This suggests that users tend to stay active on the platform for a relatively short period of time before moving on to other social media platforms."
      ],
      "metadata": {
        "id": "7ArlJ3ftHqcf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tenure refers to the amount of time a user has been active on the social media platform. In the context of this dataset, tenure represents the number of days between the user's registration date and the date when the data was collected. A higher tenure value indicates that the user has been active on the platform for a longer period of time."
      ],
      "metadata": {
        "id": "ZYtcYovcH3bK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SENTIMENT ANALYSIS"
      ],
      "metadata": {
        "id": "MSpbFaUnTE2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when\n",
        "\n",
        "# load the Facebook dataset\n",
        "df = spark.read.csv('/content/pseudo_facebook.csv', sep=',', header=True)"
      ],
      "metadata": {
        "id": "D83tUL59TEAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop rows with missing values\n",
        "df = df.na.drop()"
      ],
      "metadata": {
        "id": "RmF3PFDnTTaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert age to integer and gender to binary\n",
        "df = df.withColumn('age', df['age'].cast('integer'))\n",
        "df = df.withColumn('gender', when(col('gender') == 'male', 1).otherwise(0))"
      ],
      "metadata": {
        "id": "jBmw2en2TVPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9yy-12hTidQ",
        "outputId": "115cd5ba-9481-4ab1-cf7b-3a22165b9f8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+-------+--------+---------+------+------+------------+---------------------+-----+--------------+------------+---------------------+---------+------------------+\n",
            "| userid|age|dob_day|dob_year|dob_month|gender|tenure|friend_count|friendships_initiated|likes|likes_received|mobile_likes|mobile_likes_received|www_likes|www_likes_received|\n",
            "+-------+---+-------+--------+---------+------+------+------------+---------------------+-----+--------------+------------+---------------------+---------+------------------+\n",
            "|2094382| 14|     19|    1999|       11|     1|   266|           0|                    0|    0|             0|           0|                    0|        0|                 0|\n",
            "|1192601| 14|      2|    1999|       11|     0|     6|           0|                    0|    0|             0|           0|                    0|        0|                 0|\n",
            "|2083884| 14|     16|    1999|       11|     1|    13|           0|                    0|    0|             0|           0|                    0|        0|                 0|\n",
            "|1203168| 14|     25|    1999|       12|     0|    93|           0|                    0|    0|             0|           0|                    0|        0|                 0|\n",
            "|1733186| 14|      4|    1999|       12|     1|    82|           0|                    0|    0|             0|           0|                    0|        0|                 0|\n",
            "+-------+---+-------+--------+---------+------+------+------------+---------------------+-----+--------------+------------+---------------------+---------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the total number of likes received\n",
        "df = df.withColumn('total_likes_received', col('likes_received') + col('www_likes_received'))"
      ],
      "metadata": {
        "id": "-s05RL9TTXKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select the relevant columns for sentiment analysis\n",
        "df = df.select('userid', 'gender', 'age', 'tenure', 'friend_count', 'friendships_initiated', 'total_likes_received')\n"
      ],
      "metadata": {
        "id": "TASdnOJrTZsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show the first 5 rows of the dataframe\n",
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HY9mMCHT-6v",
        "outputId": "94581005-b896-47e3-bed4-387e981c9a6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+---+------+------------+---------------------+--------------------+\n",
            "| userid|gender|age|tenure|friend_count|friendships_initiated|total_likes_received|\n",
            "+-------+------+---+------+------------+---------------------+--------------------+\n",
            "|2094382|     1| 14|   266|           0|                    0|                 0.0|\n",
            "|1192601|     0| 14|     6|           0|                    0|                 0.0|\n",
            "|2083884|     1| 14|    13|           0|                    0|                 0.0|\n",
            "|1203168|     0| 14|    93|           0|                    0|                 0.0|\n",
            "|1733186|     1| 14|    82|           0|                    0|                 0.0|\n",
            "+-------+------+---+------+------------+---------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
        "from pyspark.ml.classification import NaiveBayes\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n"
      ],
      "metadata": {
        "id": "VSjYCAPmUAsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combine all text columns into a single column for text analysis\n",
        "df = df.withColumn(\"new_text\", col(\"userid\").cast(\"string\") + \" \" + col(\"age\").cast(\"string\") + \" \" + col(\"gender\").cast(\"string\") + \" \" + col(\"tenure\").cast(\"string\") + \" \" + col(\"friend_count\").cast(\"string\") + \" \" + col(\"friendships_initiated\").cast(\"string\") + \" \" + col(\"total_likes_received\").cast(\"string\"))\n"
      ],
      "metadata": {
        "id": "Nmy0ViusUmFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRNBmr-3V5R7",
        "outputId": "c2260c6a-0e40-4141-db41-06a6ef0bda88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.dataframe.DataFrame"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split the text into tokens\n",
        "tokenizer = Tokenizer(inputCol=\"new_text\", outputCol=\"words\")\n",
        "df = tokenizer.transform(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "XqvpOF6MUv53",
        "outputId": "b88303dc-451d-485e-a6c3-072a7268529c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-94-0519eeaaa75f>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# split the text into tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"new_text\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"words\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    260\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Input type must be string type but got double."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from pyspark.sql.functions import col\n",
        "# from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
        "# from pyspark.ml.classification import NaiveBayes\n",
        "# from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# # load the preprocessed dataframe\n",
        "# df = spark.read.csv('/path/to/your/preprocessed_dataframe.csv', header=True)\n",
        "\n",
        "# # select only the columns needed for analysis\n",
        "# df = df.select(\"userid\", \"age\", \"gender\", \"tenure\", \"friend_count\", \"friendships_initiated\", \"total_likes_received\")\n",
        "\n",
        "# # combine all text columns into a single column for text analysis\n",
        "# df = df.withColumn(\"text\", col(\"userid\") + \" \" + col(\"age\").cast(\"string\") + \" \" + col(\"gender\") + \" \" + col(\"tenure\").cast(\"string\") + \" \" + col(\"friend_count\").cast(\"string\") + \" \" + col(\"friendships_initiated\").cast(\"string\") + \" \" + col(\"total_likes_received\").cast(\"string\"))\n",
        "\n",
        "# # split the text into tokens\n",
        "# tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "# df = tokenizer.transform(df)\n",
        "\n",
        "# # remove stop words\n",
        "# remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
        "# df = remover.transform(df)\n",
        "\n",
        "# # create the Naive Bayes model\n",
        "# nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\", featuresCol=\"filtered\", labelCol=\"sentiment\")\n",
        "\n",
        "# # split the dataset into training and test sets\n",
        "# (trainingData, testData) = df.randomSplit([0.8, 0.2], seed=1234)\n",
        "\n",
        "# # train the model\n",
        "# model = nb.fit(trainingData)\n",
        "\n",
        "# # make predictions on the test set\n",
        "# predictions = model.transform(testData)\n",
        "\n",
        "# # evaluate the accuracy of the model\n",
        "# evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"sentiment\", metricName=\"accuracy\")\n",
        "# accuracy = evaluator.evaluate(predictions)\n",
        "# print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "v-YJm37dUzYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FeAcZf-JkjA7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}